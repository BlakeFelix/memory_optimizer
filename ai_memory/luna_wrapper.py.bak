import os
import requests

OLLAMA_URL = os.environ.get("OLLAMA_URL", "http://localhost:11434")
OLLAMA_MODEL = os.environ.get("OLLAMA_MODEL", "luna")

def wrap_luna_query(prompt: str, context: list[str] = None, memory_hits: list = None) -> str:
    """
    Compose the full prompt from query, memory, and context. Sends to Ollama.
    """
    ctx_block = "\n".join(context or [])
    mem_block = "\n".join(f"[MEMORY] {m['text']}" for _, m in (memory_hits or []))

    full_prompt = f"""You are Luna, a memory-augmented assistant.

Context:
{ctx_block}

Relevant memories:
{mem_block}

User query:
{prompt}
"""

    payload = {
        "model": OLLAMA_MODEL,
        "prompt": full_prompt,
        "stream": False
    }

    try:
        res = requests.post(f"{OLLAMA_URL}/api/generate", json=payload, timeout=120)
        res.raise_for_status()
        return res.json()["response"].strip()
    except Exception as e:
        return f"[luna_wrapper error] {e}"
